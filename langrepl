# Langrepl Architecture Documentation
git clone https://github.com/midodimori/langrepl.git

**Comprehensive technical documentation of the Langrepl system architecture, execution flow, and observability implementation.**
--

## Table of Contents

1. [System Overview](#system-overview)
2. [Architecture Components](#architecture-components)
3. [Execution Modes](#execution-modes)
4. [Agent Execution Flow](#agent-execution-flow)
5. [Response Rendering Pipeline](#response-rendering-pipeline)
6. [Observability & Monitoring](#observability--monitoring)
7. [Context Window Calculation](#context-window-calculation)
8. [State Management](#state-management)
9. [Tool System](#tool-system)
10. [Configuration System](#configuration-system)
11. [Key Design Patterns](#key-design-patterns)

---

## System Overview

**Langrepl** is an interactive terminal CLI for building and running LLM agents. It combines:

- **LangChain/LangGraph**: Agent orchestration and execution
- **Prompt Toolkit**: Rich terminal input interface (REPL)
- **Rich**: Beautiful output rendering (Markdown, syntax highlighting)
- **Pydantic**: Configuration and data validation
- **SQLite**: Persistent conversation storage

### Core Philosophy

```
Input (prompt_toolkit) → Execution (LangGraph) → Rendering (Rich)
```

The architecture maintains clean separation between:
1. **Input Layer**: User interaction and command handling
2. **Execution Layer**: Agent graph compilation and streaming
3. **Output Layer**: Response formatting and display
4. **Persistence Layer**: Thread/checkpoint storage

---

## Architecture Components

### 1. CLI Layer (`src/cli/`)

The user-facing interactive interface.

#### **Core Modules**

**`app.py`** - Application entry point
- Command-line argument parsing
- Mode routing (chat vs server)
- Async execution wrapper

**`session.py`** - Session management
- Graph lifecycle management
- Main REPL loop
- Context synchronization
- Reload handling

**`context.py`** - Runtime context
- Thread management
- Token/cost tracking
- Approval mode state
- Configuration propagation

**`chat.py`** - Chat command handler
- Session initialization
- Resume functionality
- Working directory setup

**`server.py`** - LangGraph server mode
- `langgraph.json` generation
- Server subprocess management
- Assistant API integration

#### **Interface Modules** (`src/cli/interface/`)

**`prompt.py`** - Interactive input (prompt_toolkit)
- `PromptSession` configuration
- Auto-completion for slash commands
- History management
- Multiline validation
- Key bindings (Ctrl+C, Ctrl+J, Shift+Tab)
- Dynamic bottom toolbar
- Approval mode styling

**`renderer.py`** - Output rendering (Rich)
- AI message rendering with thinking extraction
- Tool call formatting
- Markdown/code block rendering
- Message type routing
- Graph visualization (Mermaid)

**`messages.py`** - Message handling
- User message processing
- Graph streaming orchestration
- Chunk processing and deduplication
- Interrupt handling loop
- Auto-compression triggers

**`commands.py`** - Slash command dispatcher
- Command registration
- Dynamic help generation
- Argument parsing and validation

**`models.py`** - Model selection UI
- Interactive model picker (prompt_toolkit)
- Agent/subagent selection
- Arrow key navigation

**`interrupts.py`** - Tool approval system
- Approval prompt UI
- Rule-based auto-approval
- Resume value handling

**`compress.py`** - Conversation compression
- Context window monitoring
- Thread summarization
- New thread creation

**`graph.py`** - Graph visualization
- Mermaid diagram rendering
- PNG generation (browser mode)
- ASCII terminal output

**`memory.py`** - User memory management
- Project-specific instructions
- Memory file editing
- Persistence handling

**`tools.py`** - Tool management UI
- Tool inspection
- Interactive selection

**`resume.py`** - Thread resumption
- Thread selection UI
- History replay

**`replay.py`** - Message replay
- Historical conversation display

#### **Theme System** (`src/cli/theme/`)

**`base.py`** - Theme protocol
**`tokyo_night.py`** - Default theme implementation
**`console.py`** - Rich console wrapper with theme support
**`registry.py`** - Theme registration and loading

---

### 2. Agent Layer (`src/agents/`)

LangGraph agent implementation.

**`factory.py`** - Graph and agent factories
- `AgentFactory`: Creates ReAct or Deep agents
- `GraphFactory`: Compiles state graphs with tools
- Tool filtering and module mapping
- Subagent creation
- Prompt template rendering

**`react_agent.py`** - ReAct agent implementation
- `create_react_agent()`: Main agent builder
- `CompressingToolNode`: Tool output compression
- Token usage extraction
- Cost calculation integration
- Error handling and formatting

**`deep_agent.py`** - Deep agent implementation
- Subagent delegation via task tools
- Planning capabilities
- Virtual filesystem integration

---

### 3. State Management (`src/state/`)

**`base.py`** - Base state schema
```python
class BaseState(AgentStatePydantic):
    todos: list[Todo] | None
    files: dict[str, str]  # Virtual filesystem
    current_input_tokens: int | None
    current_output_tokens: int | None
    total_cost: float | None
```

State is propagated through:
1. Agent graph execution
2. CLI context synchronization
3. Checkpointer persistence
4. UI display updates

---

### 4. Tool System (`src/tools/`)

**`factory.py`** - Tool factory
- Loads impl, MCP, and internal tools
- Tool wrapping with approval
- Glob pattern filtering

**`wrapper.py`** - Tool approval wrapper
- Regex-based allow/deny rules
- Interactive approval prompts
- Mode-based bypassing

**Implementation Tools** (`src/tools/impl/`)
- `file_system.py`: File operations
- `grep_search.py`: ripgrep integration
- `terminal.py`: Shell command execution
- `web.py`: Web scraping (trafilatura)

**Internal Tools** (`src/tools/internal/`)
- Memory operations
- Virtual filesystem

**Subagent Tools** (`src/tools/subagents/`)
- `thinking.py`: Subagent delegation
- Task routing to specialized agents

---

### 5. LLM Layer (`src/llms/`)

**`factory.py`** - LLM factory
- Multi-provider support (OpenAI, Anthropic, Google, Bedrock, etc.)
- Rate limiting configuration
- Model instantiation

**Wrappers** (`src/llms/wrappers/`)
- `zhipuai.py`: ZhipuAI reasoning content extraction

---

### 6. MCP Integration (`src/mcp/`)

**`factory.py`** - MCP factory
**`client.py`** - MCP client
- Server lifecycle management
- Tool discovery and wrapping
- SSE/stdio transport support

---

### 7. Checkpointer Layer (`src/checkpointer/`)

**`factory.py`** - Checkpointer factory
**`utils.py`** - Helper utilities
**Implementation** (`src/checkpointer/impl/`)
- `sqlite.py`: Persistent SQLite storage
- `memory.py`: In-memory checkpointing (server mode)

---

### 8. Configuration System (`src/core/`)

**`config.py`** - Configuration models
- `LLMConfig`: Model configuration
- `AgentConfig`: Agent configuration
- `MCPConfig`: MCP server configuration
- `CheckpointerConfig`: Storage configuration
- `ApprovalMode`: Tool approval modes

**`settings.py`** - Application settings (Pydantic)
```python
class Settings:
    log_level: str
    llm: LLMSettings
    tool_settings: ToolSettings
    cli: CLISettings
    server: ServerSettings
```

Environment variable loading:
- Nested delimiter: `__` (e.g., `LLM__OPENAI_API_KEY`)
- Case-insensitive
- `.env` file support

**`constants.py`** - System constants
**`logging.py`** - Logging configuration

---

### 9. Utilities (`src/utils/`)

**`cost.py`** - Cost calculation
- Token-to-cost conversion
- Context percentage calculation
- Formatting helpers

**`compression.py`** - Message compression
- Token counting
- Compression threshold checking

**`render.py`** - Template rendering
- Jinja2 template support
- Context variable injection

**`time.py`** - Time utilities
**`file.py`** - File operations
**`bash.py`** - Shell command helpers
**`rate_limiter.py`** - Rate limiting
**`path.py`** - Path utilities

---

## Execution Modes

### 1. Direct Execution Mode (Primary)

**Command:**
```bash
langrepl              # Default agent
langrepl -a general   # Specific agent
langrepl -r           # Resume last thread
```

**Flow:**
```
CLI Entry (app.py:cli)
    ↓
main() → handle_chat_command()
    ↓
Context.create()  ← Load configs, create/resume thread
    ↓
CLISession(context)
    ↓
initializer.get_graph()
    ├─ Load agent config (YAML)
    ├─ Load LLM config (YAML)
    ├─ Load MCP config (JSON)
    ├─ Create checkpointer (SQLite)
    ├─ GraphFactory.create()
    │   ├─ Load tools (impl, MCP, internal)
    │   ├─ Create main agent
    │   ├─ Create subagents
    │   └─ Compile StateGraph
    └─ Return CompiledStateGraph
    ↓
session.start()
    ↓
_main_loop()
    ├─ prompt.get_input()  ← prompt_toolkit
    ├─ command_handler.handle()  OR
    └─ message_handler.handle()
        ↓
        graph.astream(input, config)  ← EXECUTE IN-PROCESS
            ↓
        Stream chunks → renderer.render_message()
```

**Key Characteristics:**
- ✅ Runs in single process
- ✅ Uses configured checkpointer (persistent threads)
- ✅ Full CLI features (resume, compression, etc.)
- ✅ Real-time streaming output
- ✅ Interactive tool approval

---

### 2. LangGraph Server Mode (Optional)

**Command:**
```bash
langrepl -s -a general
```

**Flow:**
```
CLI Entry (app.py:cli)
    ↓
main() → handle_server_command()
    ↓
generate_langgraph_json()
    ├─ Create langgraph.json config
    └─ Reference: src/cli/core/server.py:get_graph
    ↓
Set environment variables
    ├─ LANGREPL_AGENT=general
    ├─ LANGREPL_MODEL=...
    └─ LANGREPL_WORKING_DIR=/path
    ↓
subprocess.Popen([
    "uv", "run", "langgraph", "dev",
    "--config", "langgraph.json"
])
    ↓
LangGraph CLI loads server.py:get_graph()
    ├─ Reads env vars
    ├─ Calls initializer.get_graph()
    └─ Returns CompiledStateGraph
    ↓
LangGraph HTTP Server (localhost:2024)
    ├─ API endpoints (/runs, /threads, etc.)
    ├─ LangGraph Studio integration
    └─ In-memory checkpointing (ephemeral)
    ↓
Create/update assistant via API
    └─ Ready for HTTP requests
```

**Generated `langgraph.json`:**
```json
{
  "dependencies": ["/path/to/langrepl"],
  "graphs": {
    "agent": "src/cli/core/server.py:get_graph"
  },
  "env": ".env"
}
```

**Key Characteristics:**
- ❌ NOT in CLI process (subprocess)
- ⚠️ In-memory checkpointing only (ephemeral threads)
- ✅ HTTP API access
- ✅ LangGraph Studio visual debugging
- ✅ Assistant management API

---

## Agent Execution Flow

### Graph Compilation

**1. Configuration Loading** (`initializer.py`)
```python
async def get_graph(agent, model, working_dir):
    # Load YAML configs
    agent_config = await load_agent_config(agent, working_dir)
    llm_config = await load_llm_config(model, working_dir)
    mcp_config = await load_mcp_config(working_dir)
    
    # Create checkpointer
    checkpointer = checkpointer_factory.create(
        agent_config.checkpointer,
        db_path
    )
    
    # Create graph
    graph = await graph_factory.create(
        agent_config,
        BaseState,
        mcp_config,
        llm_config,
        template_context
    )
    
    # Compile with checkpointer
    compiled = graph.compile(checkpointer=checkpointer)
    return compiled
```

**2. Tool Loading** (`GraphFactory.create`)
```python
# Load all available tools
all_impl_tools = tool_factory.get_impl_tools()
all_mcp_tools = await mcp_client.get_mcp_tools()
all_internal_tools = tool_factory.get_internal_tools()

# Filter by patterns (glob matching)
tools = filter_tools(all_impl_tools, impl_patterns)
tools += filter_tools(all_mcp_tools, mcp_patterns)
internal_tools = filter_tools(all_internal_tools, internal_patterns)
```

**3. Agent Creation** (`AgentFactory.create`)
```python
def create(self, tools, llm, prompt, subagents=None):
    if subagents:
        # Deep agent with subagent delegation
        return create_deep_agent(
            tools, prompt, llm, subagents, ...
        )
    else:
        # ReAct agent
        return create_react_agent(
            tools, prompt, llm, ...
        )
```

**4. Graph Structure** (`create_react_agent`)
```python
workflow = StateGraph(state_schema)

# Add nodes
workflow.add_node("agent", acall_model)
workflow.add_node("tools", CompressingToolNode(tools, model))

# Define edges
workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.add_conditional_edges("tools", route_tool_responses, ["agent", END])

# Compile
return workflow.compile(checkpointer=checkpointer, name=name)
```

### Streaming Execution

**MessageHandler.handle()** (`src/cli/interface/messages.py`)

```python
async def handle(self, content: str):
    # 1. Create input
    human_message = HumanMessage(content=content)
    
    # 2. Prepare config
    config = RunnableConfig(
        configurable={
            "thread_id": ctx.thread_id,
            "approval_mode": ctx.approval_mode.value,
            "working_dir": ctx.working_dir,
            "input_cost_per_mtok": ctx.input_cost_per_mtok,
            "output_cost_per_mtok": ctx.output_cost_per_mtok,
            "tool_output_max_tokens": ctx.tool_output_max_tokens,
        },
        recursion_limit=ctx.recursion_limit,
    )
    
    # 3. Stream response
    await self._stream_response(
        {"messages": [human_message]},
        config
    )
```

**Streaming Loop** (`_stream_response`)

```python
async def _stream_response(self, input_data, config):
    current_input = input_data
    rendered_messages = set()
    
    while True:
        interrupted = False
        
        # Stream chunks from graph
        async for chunk in self.session.graph.astream(
            current_input,
            config,
            stream_mode="updates",  # Get state updates
            subgraphs=True,         # Include subagent outputs
        ):
            # Check for interrupts (tool approvals)
            interrupts = self._extract_interrupts(chunk)
            if interrupts:
                # Handle approval, get resume value
                resume_value = await interrupt_handler.handle(interrupts)
                current_input = Command(resume=resume_value)
                interrupted = True
                break
            else:
                # Process and render chunk
                await self._handle_chunk(chunk, rendered_messages)
        
        if not interrupted:
            break  # Streaming completed
```

**Chunk Processing** (`_handle_chunk`)

```python
async def _handle_chunk(self, chunk, rendered_messages):
    if isinstance(chunk, tuple) and len(chunk) == 2:
        namespace, data = chunk  # (namespace, node_updates)
        
        for node_name, node_data in data.items():
            if "messages" in node_data:
                last_message = node_data["messages"][-1]
                
                # Deduplication
                message_id = f"{last_message.id}_{last_message.type}"
                if message_id in rendered_messages:
                    return
                rendered_messages.add(message_id)
                
                # Render message
                self.session.renderer.render_message(last_message)
                
                # Sync context (tokens, cost)
                if isinstance(last_message, AIMessage):
                    self.session.update_context(
                        current_input_tokens=node_data.get("current_input_tokens"),
                        current_output_tokens=node_data.get("current_output_tokens"),
                        total_cost=node_data.get("total_cost"),
                    )
                    
                    # Check auto-compression
                    await self._check_auto_compression()
```

### Agent Node Execution

**`acall_model()` in react_agent.py**

```python
async def acall_model(state, config):
    try:
        # Get input messages
        model_input = _get_model_input_state(state)
        
        # Invoke LLM
        response = await static_model.ainvoke(model_input, config)
        response.name = name  # Add agent name
        
        # Check recursion limit
        if _are_more_steps_needed(state, response):
            return {"messages": [AIMessage(
                content="Sorry, need more steps...",
                is_error=True
            )]}
        
        # Extract usage metadata
        result = {"messages": [response]}
        usage_metadata = getattr(response, "usage_metadata", None)
        if usage_metadata:
            result["current_input_tokens"] = usage_metadata.get("input_tokens")
            result["current_output_tokens"] = usage_metadata.get("output_tokens")
            
            # Calculate cost
            input_cost = config.get("configurable", {}).get("input_cost_per_mtok")
            output_cost = config.get("configurable", {}).get("output_cost_per_mtok")
            if input_cost and output_cost:
                call_cost = calculate_cost(
                    usage_metadata.get("input_tokens", 0),
                    usage_metadata.get("output_tokens", 0),
                    input_cost,
                    output_cost
                )
                current_total = state.total_cost or 0
                result["total_cost"] = current_total + call_cost
        
        return result
        
    except Exception as e:
        return {"messages": [AIMessage(
            content=f"{type(e).__name__}: {str(e)}",
            is_error=True
        )]}
```

### Tool Execution

**CompressingToolNode** (`react_agent.py`)

```python
async def _arun_one(self, call, input_type, tool_runtime):
    # Execute tool normally
    result = await super()._arun_one(call, input_type, tool_runtime)
    
    if isinstance(result, Command):
        return result  # Pass through
    
    tool_msg = cast(ToolMessage, result)
    
    # Get max_tokens from config
    max_tokens = tool_runtime.config.get("configurable", {}).get("tool_output_max_tokens")
    if not max_tokens:
        return tool_msg
    
    # Check if output exceeds limit
    content = tool_msg.content
    if isinstance(content, str) and content.strip():
        token_count = calculate_message_tokens([HumanMessage(content=content)], self.model)
        
        if token_count > max_tokens:
            # Compress to virtual filesystem
            file_id = f"tool_output_{tool_msg.tool_call_id}.txt"
            
            # Store original content
            compressed_msg = ToolMessage(
                content=f"Output compressed to virtual file: {file_id}\n"
                        f"Use read_memory_file(file_id='{file_id}') to read full content.",
                tool_call_id=tool_msg.tool_call_id,
                short_content="[Output compressed to virtual file]"
            )
            
            # Return Command with virtual file update
            return Command(
                update={"messages": [compressed_msg], "files": {file_id: content}}
            )
    
    return tool_msg
```

---

## Response Rendering Pipeline

### Message Rendering

**Renderer.render_message()** (`src/cli/interface/renderer.py`)

```python
def render_message(message: AnyMessage):
    if isinstance(message, HumanMessage):
        render_user_message(message)
    elif isinstance(message, AIMessage):
        render_assistant_message(message)
    elif isinstance(message, ToolMessage):
        render_tool_message(message)
```

### AI Message Rendering

**Multi-source thinking extraction:**

```python
def render_assistant_message(message: AIMessage):
    content = message.content
    tool_calls = message.tool_calls
    
    # 1. Extract thinking from metadata (Bedrock)
    thinking_parts = []
    metadata_thinking = _extract_thinking_from_metadata(message)
    if metadata_thinking:
        thinking_parts.append(metadata_thinking)
    
    # 2. Extract from content blocks (Anthropic structured output)
    if isinstance(content, list):
        text_parts, block_thinking = _extract_thinking_and_text_from_blocks(content)
        thinking_parts.extend(block_thinking)
        content = "".join(text_parts)
    
    # 3. Extract XML-style tags (<think>...</think>)
    if isinstance(content, str):
        content, xml_thinking = _extract_thinking_tags(content)
        if xml_thinking:
            thinking_parts.append(xml_thinking)
    
    # Render thinking (italic dim)
    parts = []
    if thinking_parts:
        parts.append(Text("\n\n".join(thinking_parts), style="italic dim"))
        parts.append(NewLine())
    
    # Render main content (Markdown)
    content = _fix_malformed_code_blocks(content)
    if content:
        parts.append(TransparentMarkdown(content, code_theme="dracula"))
    
    # Print
    if parts:
        console.print(Group(*parts))
    
    # Render tool calls (muted)
    if tool_calls:
        if parts:
            console.print(NewLine())
        for tool_call in tool_calls:
            console.print(Text(_format_tool_call(tool_call), style="muted"))
```

### Code Block Rendering

**TransparentMarkdown** - Custom Rich Markdown with transparent code blocks

```python
class TransparentSyntax(Syntax):
    """Syntax with transparent background (no theme background)."""
    
    @classmethod
    def get_theme(cls, name):
        base_theme = super().get_theme(name)
        
        class TransparentThemeWrapper(SyntaxTheme):
            def get_style_for_token(self, token_type):
                style = base_theme.get_style_for_token(token_type)
                return Style(
                    color=style.color,
                    bold=style.bold,
                    italic=style.italic,
                    underline=style.underline,
                    # No background!
                )
            
            def get_background_style(self):
                return Style()  # Transparent
        
        return TransparentThemeWrapper(base_theme)
```

---

## Observability & Monitoring

### 1. Logging System

**Configuration** (`src/core/logging.py`)

```python
def configure_logging(show_logs: bool = False):
    """Configure application logging."""
    
    # Format
    LOG_FORMAT = "%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s"
    formatter = logging.Formatter(LOG_FORMAT)
    
    # Root logger
    root_logger = logging.getLogger()
    root_logger.setLevel(settings.log_level)  # From env: LOG_LEVEL
    
    # Suppress noisy libraries
    logging.getLogger("langchain_google_genai._function_utils").setLevel(logging.ERROR)
    logging.getLogger("langchain_anthropic").setLevel(logging.ERROR)
    logging.getLogger("langchain_openai").setLevel(logging.ERROR)
    
    # Console handler (optional)
    if show_logs:
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        root_logger.addHandler(console_handler)
```

**Usage Pattern:**

```python
from src.core.logging import get_logger

logger = get_logger(__name__)

logger.debug("Detailed debug info")
logger.info("General info")
logger.warning("Warning message")
logger.error("Error occurred")
logger.exception("Error with traceback")
```

**Environment Configuration:**
```bash
LOG_LEVEL=INFO        # DEBUG, INFO, WARNING, ERROR
```

**Logging Locations:**
- Most modules use `logger.debug()` for diagnostic info
- Errors use `logger.exception()` for full traceback
- User-facing errors shown via `console.print_error()`
- Logs hidden by default (clean UX), enabled via `show_logs=True`

---

### 2. LangSmith Tracing

**Integration: Automatic via environment variables**

```bash
# .env configuration
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT="https://api.smith.langchain.com"
LANGCHAIN_API_KEY="your_langsmith_api_key"
LANGCHAIN_PROJECT="your_project_name"
```

**How it works:**

1. **Automatic Instrumentation**: LangChain automatically traces:
   - LLM invocations
   - Tool executions
   - Graph node transitions
   - Errors and retries

2. **Trace Hierarchy**:
   ```
   Thread
   ├─ User Input
   ├─ Agent Node
   │  └─ LLM Call (with tokens, latency)
   ├─ Tool Node
   │  ├─ Tool 1 execution
   │  └─ Tool 2 execution
   └─ Agent Node (response)
   ```

3. **Captured Metrics**:
   - Input/output tokens
   - Latency per node
   - Error traces
   - Tool call arguments and results
   - Full conversation history

4. **No Code Changes Required**: Tracing is passive when env vars are set

**LangSmith Features:**
- Visual trace inspection
- Playground (replay/modify runs)
- Dataset creation from traces
- Evaluation runners
- Cost tracking

---

### 3. Token Usage Tracking

**Implementation: Real-time extraction from LLM responses**

**Agent-level tracking** (`react_agent.py:acall_model`)

```python
# After LLM invocation
response = await model.ainvoke(input, config)

# Extract usage metadata
usage_metadata = getattr(response, "usage_metadata", None)
if usage_metadata:
    result["current_input_tokens"] = usage_metadata.get("input_tokens")
    result["current_output_tokens"] = usage_metadata.get("output_tokens")
```

**State propagation** (`BaseState`)

```python
class BaseState(AgentStatePydantic):
    current_input_tokens: int | None = None
    current_output_tokens: int | None = None
    total_cost: float | None = None
```

**CLI synchronization** (`messages.py:_handle_chunk`)

```python
if isinstance(last_message, AIMessage):
    self.session.update_context(
        current_input_tokens=node_data.get("current_input_tokens"),
        current_output_tokens=node_data.get("current_output_tokens"),
        total_cost=node_data.get("total_cost"),
    )
```

**Display** (`prompt.py:_get_placeholder`)

```python
if ctx.current_input_tokens and ctx.current_output_tokens:
    total_tokens = ctx.current_input_tokens + ctx.current_output_tokens
    tokens_formatted = format_tokens(total_tokens)  # "123K", "1.2M"
    window_formatted = format_tokens(ctx.context_window)
    percentage = calculate_context_percentage(total_tokens, ctx.context_window)
    
    usage_info = f"[{tokens_formatted}/{window_formatted} tokens ({percentage:.0f}%)"
    
    if ctx.total_cost:
        usage_info += f" | {format_cost(ctx.total_cost)}"  # "$1.23"
    
    usage_info += "]"
```

**Result: Live token/cost display in prompt**
```
general:gpt-4  [45K/128K tokens (35%) | $0.12]
❯ 
```

---

### 4. Cost Calculation

**Configuration** (`config.llms.yml`)

```yaml
llms:
  - alias: gpt-4
    provider: openai
    model: gpt-4
    context_window: 128000
    input_cost_per_mtok: 30.0    # $30 per million input tokens
    output_cost_per_mtok: 60.0   # $60 per million output tokens
```

**Calculation** (`utils/cost.py`)

```python
def calculate_cost(input_tokens, output_tokens, input_cost_per_mtok, output_cost_per_mtok):
    """Calculate cost in dollars."""
    input_cost = (input_tokens / 1_000_000) * input_cost_per_mtok
    output_cost = (output_tokens / 1_000_000) * output_cost_per_mtok
    return input_cost + output_cost
```

**Accumulation** (`react_agent.py`)

```python
if input_cost and output_cost:
    call_cost = calculate_cost(
        usage_metadata.get("input_tokens", 0),
        usage_metadata.get("output_tokens", 0),
        input_cost,
        output_cost
    )
    current_total = state.total_cost or 0
    result["total_cost"] = current_total + call_cost  # Cumulative
```

**Persistence**: Costs stored in state, persisted via checkpointer

---

### 5. Performance Timing

**Timer utility** (`src/cli/timer.py`)

```python
from src.cli.timer import timer, enable_timer

# Enable via CLI flag
langrepl -t  # or --timer

# Usage in code
with timer("Load configs"):
    config = await load_config()

# Output: ⏱  Load configs: 0.123s
```

**Timed operations:**
- Config loading
- Graph compilation
- Checkpointer creation
- MCP client initialization

**Example output:**
```
⏱  Load configs: 0.234s
⏱  Create checkpointer: 0.045s
⏱  Create graph: 1.234s
⏱  Compile graph: 0.567s
```

---

### 6. Context Window Monitoring

**Auto-compression trigger** (`messages.py:_check_auto_compression`)

```python
async def _check_auto_compression(self):
    # Get compression config
    compression_config = agent_config.compression
    if not compression_config.auto_compress_enabled:
        return
    
    context_window = agent_config.llm.context_window
    current_tokens = ctx.current_input_tokens or 0
    
    # Check threshold
    if should_auto_compress(current_tokens, context_window, threshold):
        usage_pct = int((current_tokens / context_window * 100))
        
        with console.status(f"Context at {usage_pct}%, auto-compressing..."):
            compression_handler = CompressionHandler(self.session)
            await compression_handler.handle()
```

**Threshold check** (`utils/compression.py`)

```python
def should_auto_compress(current_tokens, context_window, threshold):
    """Check if tokens exceed threshold percentage."""
    if context_window <= 0:
        return False
    usage_ratio = current_tokens / context_window
    return usage_ratio >= threshold  # e.g., 0.8 = 80%
```

**Configuration** (`config.agents.yml`)

```yaml
agents:
  - name: general
    compression:
      auto_compress_enabled: true
      auto_compress_threshold: 0.8  # 80%
      compression_llm: haiku-4.5    # Cheaper model for compression
```

**Compression flow:**
1. Monitor `current_input_tokens` vs `context_window`
2. When threshold exceeded (e.g., 80%):
   - Create summary of conversation
   - Create new thread with summary as context
   - Switch to new thread
   - Continue conversation

---

### 7. Error Handling

**Layered error handling:**

**1. Tool Execution** (`react_agent.py:_format_tool_error`)
```python
def _format_tool_error(e: Exception) -> str:
    if isinstance(e, ToolException):
        return str(e)  # Clean message
    
    safe_exceptions = (ValueError, TypeError, KeyError, ...)
    if isinstance(e, safe_exceptions):
        return f"{type(e).__name__}: {e}"
    
    return f"{type(e).__name__}: An error occurred"
```

**2. Agent Execution** (`react_agent.py:acall_model`)
```python
try:
    response = await model.ainvoke(input, config)
    return {"messages": [response]}
except Exception as e:
    return {"messages": [AIMessage(
        content=f"{type(e).__name__}: {str(e)}",
        is_error=True
    )]}
```

**3. CLI Level** (`session.py:_main_loop`)
```python
try:
    user_input = await self.prompt.get_input()
    await self.message_handler.handle(user_input)
except EOFError:
    break
except Exception as e:
    console.print_error(f"Error processing input: {e}")
    logger.debug("Input processing error")  # Full trace
```

**Error display:**
- User-facing: `console.print_error()` (red text)
- Developer: `logger.exception()` (full traceback)
- AI messages: `is_error=True` flag for styling

---

## Context Window Calculation

### Overview

The system tracks token usage against model context limits to prevent overflow and trigger automatic compression. **Context window size is static (configured), while current token usage is dynamic (calculated).**

---

### 1. Context Window: Static Configuration

The context window size represents the **maximum token capacity** of the LLM and is **manually configured** based on provider documentation.

#### **Configuration Source**

`config.llms.yml`:
```yaml
llms:
  - alias: gpt-4
    provider: openai
    model: gpt-4
    context_window: 128000  # ← Static: 128K tokens (from OpenAI docs)
    input_cost_per_mtok: 30.0
    output_cost_per_mtok: 60.0

  - alias: claude-sonnet
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    context_window: 200000  # ← Static: 200K tokens (from Anthropic docs)
    input_cost_per_mtok: 3.0
    output_cost_per_mtok: 15.0

  - alias: gemini-2.5-pro
    provider: google
    model: gemini-2.5-pro
    context_window: 1000000  # ← Static: 1M tokens (from Google docs)
    input_cost_per_mtok: 0.10
    output_cost_per_mtok: 0.40
```

#### **Propagation to Runtime**

```python
# src/cli/core/context.py
@classmethod
async def create(cls, agent, model, resume, working_dir):
    llm_config = await initializer.load_llm_config(model, working_dir)
    
    return cls(
        agent=agent,
        model=model,
        context_window=llm_config.context_window,  # ← Static from config
        current_input_tokens=None,  # ← Will be updated dynamically
        current_output_tokens=None,
    )
```

**Key Characteristics:**
- ✅ Set once from configuration files
- ✅ Based on official provider documentation
- ✅ Represents maximum model capacity
- ✅ Never changes during session

---

### 2. Current Token Usage: Real-Time Calculation

The **actual token count** of the conversation is calculated dynamically using multiple methods.

#### **Method 1: LLM Response Metadata (Primary)**

Most accurate - uses the LLM provider's own token count.

```python
# src/agents/react_agent.py - acall_model()
async def acall_model(state, config):
    # Invoke LLM with conversation history
    response = await static_model.ainvoke(model_input, config)
    
    # Extract usage metadata from response
    usage_metadata = getattr(response, "usage_metadata", None)
    if usage_metadata:
        input_tokens = usage_metadata.get("input_tokens")    # What LLM counted
        output_tokens = usage_metadata.get("output_tokens")  # What it generated
        
        # Update state
        result = {
            "messages": [response],
            "current_input_tokens": input_tokens,
            "current_output_tokens": output_tokens,
        }
    
    return result
```

**Why this is accurate:**
- Uses the **LLM provider's actual tokenizer**
- Accounts for special tokens, formatting, system messages
- Reflects what you're **actually billed for**
- Different models have different tokenization schemes

#### **Method 2: Manual Token Calculation (Fallback)**

For tool outputs or when LLM doesn't provide metadata.

```python
# src/utils/compression.py
def calculate_message_tokens(messages, llm):
    """Calculate token count with multi-level fallback."""
    
    # Level 1: Use LLM's native token counter (best)
    try:
        return llm.get_num_tokens_from_messages(list(messages))
    except (NotImplementedError, ImportError):
        pass
    
    # Level 2: Use tiktoken - OpenAI's tokenizer (good)
    try:
        encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4/3.5 encoding
        content = " ".join(msg.text for msg in messages)
        return len(encoding.encode(content))
    except Exception:
        pass
    
    # Level 3: Character-based estimation (rough)
    # Approximation: 1 token ≈ 4 characters
    content = " ".join(msg.text for msg in messages)
    return len(content) // 4
```

**Fallback Strategy:**
1. **Best**: LLM's native `get_num_tokens_from_messages()` method
2. **Good**: `tiktoken` library with `cl100k_base` encoding
3. **Rough**: Character count ÷ 4 (1 token ≈ 4 chars)

---

### 3. Token Tracking Flow

Complete flow from LLM invocation to display:

```
┌─────────────────────────────────────────────────────────────┐
│ 1. CONFIGURATION (Static - Set Once)                       │
├─────────────────────────────────────────────────────────────┤
│ config.llms.yml:                                            │
│   context_window: 128000  ← From provider docs             │
└─────────────────┬───────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────────┐
│ 2. CONTEXT INITIALIZATION                                   │
├─────────────────────────────────────────────────────────────┤
│ Context.create():                                           │
│   llm_config = load_llm_config(model)                      │
│   context.context_window = 128000                          │
└─────────────────┬───────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────────┐
│ 3. AGENT EXECUTION (Real-time)                              │
├─────────────────────────────────────────────────────────────┤
│ User: "Analyze this code"                                   │
│   ↓                                                          │
│ Agent invokes LLM with full conversation history           │
│   ↓                                                          │
│ LLM Response includes usage_metadata:                       │
│   {                                                          │
│       "input_tokens": 1234,    ← LLM's actual count        │
│       "output_tokens": 567                                  │
│   }                                                          │
│   ↓                                                          │
│ State Update:                                               │
│   current_input_tokens = 1234                              │
│   current_output_tokens = 567                              │
└─────────────────┬───────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────────┐
│ 4. CLI CONTEXT SYNC                                         │
├─────────────────────────────────────────────────────────────┤
│ session.update_context(                                     │
│     current_input_tokens=1234,                             │
│     current_output_tokens=567                              │
│ )                                                            │
└─────────────────┬───────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────────┐
│ 5. DISPLAY CALCULATION                                      │
├─────────────────────────────────────────────────────────────┤
│ total_tokens = 1234 + 567 = 1801                           │
│ context_window = 128000                                     │
│ percentage = (1801 / 128000) * 100 = 1.4%                  │
│                                                              │
│ Format and display in prompt:                               │
│   [1.8K/128K tokens (1%) | $0.02]                          │
│   ❯ _                                                       │
└─────────────────┬───────────────────────────────────────────┘
                  │
                  ▼
┌─────────────────────────────────────────────────────────────┐
│ 6. AUTO-COMPRESSION CHECK                                   │
├─────────────────────────────────────────────────────────────┤
│ threshold = 0.8  # 80% from config                         │
│ usage_ratio = 1801 / 128000 = 0.014                        │
│                                                              │
│ if usage_ratio >= 0.8:                                      │
│     # Not yet - only 1.4% used                             │
│     pass                                                     │
│                                                              │
│ Later, when tokens = 102400:                                │
│ usage_ratio = 102400 / 128000 = 0.8                        │
│                                                              │
│ if usage_ratio >= 0.8:                                      │
│     # TRIGGER COMPRESSION!                                  │
│     compress_and_create_new_thread()                        │
└─────────────────────────────────────────────────────────────┘
```

---

### 4. Display Implementation

#### **Prompt Placeholder**

Live token/cost display in the CLI prompt:

```python
# src/cli/interface/prompt.py
def _get_placeholder(self):
    ctx = self.context
    
    if ctx.current_input_tokens and ctx.current_output_tokens:
        # Calculate total usage
        total_tokens = ctx.current_input_tokens + ctx.current_output_tokens
        
        # Format numbers
        tokens_formatted = format_tokens(total_tokens)        # "1.8K" or "1.2M"
        window_formatted = format_tokens(ctx.context_window)  # "128K"
        
        # Calculate percentage
        percentage = calculate_context_percentage(total_tokens, ctx.context_window)
        # (1801 / 128000) * 100 = 1.4%
        
        usage_info = f"[{tokens_formatted}/{window_formatted} tokens ({percentage:.0f}%)"
        
        # Add cost if available
        if ctx.total_cost:
            usage_info += f" | {format_cost(ctx.total_cost)}"  # "$0.02"
        
        usage_info += "]"
    
    return HTML(f"<placeholder>{agent_name}{usage_info}</placeholder>")
```

**Example displays:**
```
general:gpt-4  [1.8K/128K tokens (1%) | $0.02]
❯ _

general:claude-sonnet  [45K/200K tokens (23%) | $0.34]
❯ _

general:gemini-2.5-pro  [234K/1.0M tokens (23%) | $0.12]
❯ _
```

---

### 5. Auto-Compression Mechanism

Prevents context overflow by automatically creating new threads when threshold is exceeded.

#### **Configuration**

```yaml
# config.agents.yml
agents:
  - name: general
    compression:
      auto_compress_enabled: true
      auto_compress_threshold: 0.8  # 80% of context window
      compression_llm: haiku-4.5    # Cheaper model for summarization
```

#### **Monitoring Logic**

```python
# src/cli/interface/messages.py
async def _check_auto_compression(self):
    # Get configuration
    context_window = agent_config.llm.context_window  # Static: 128000
    current_tokens = ctx.current_input_tokens or 0     # Dynamic: 102400
    threshold = compression_config.auto_compress_threshold  # 0.8
    
    # Check if threshold exceeded
    if should_auto_compress(current_tokens, context_window, threshold):
        usage_pct = int((current_tokens / context_window * 100))  # 80%
        
        # Trigger compression
        with console.status(f"Context at {usage_pct}%, auto-compressing..."):
            await compression_handler.handle()
```

```python
# src/utils/compression.py
def should_auto_compress(current_tokens, context_window, threshold):
    """Check if auto-compression should be triggered."""
    if context_window is None or context_window <= 0:
        return False
    
    usage_ratio = current_tokens / context_window
    return usage_ratio >= threshold  # e.g., 0.8 >= 0.8
```

#### **Compression Flow**

When threshold is exceeded:
1. **Create summary** using cheaper LLM (e.g., haiku-4.5)
2. **Create new thread** with summary as context
3. **Switch to new thread** seamlessly
4. **Continue conversation** with fresh context window

**Example:**
```
Before compression:
  [102K/128K tokens (80%)]  ← Threshold reached!

Compressing...
  ⏱ Summarizing conversation with haiku-4.5
  ⏱ Creating new thread
  ⏱ Switching context

After compression:
  [5.2K/128K tokens (4%)]  ← Fresh start with summary
```

---

### 6. Persistence

Token counts and costs are persisted across sessions.

#### **State Storage**

```python
# src/state/base.py
class BaseState(AgentStatePydantic):
    messages: list[AnyMessage]
    current_input_tokens: int | None = None    # ← Persisted
    current_output_tokens: int | None = None   # ← Persisted
    total_cost: float | None = None            # ← Persisted
```

#### **SQLite Checkpointer**

```python
# src/checkpointer/impl/sqlite.py
async def aput(self, config, checkpoint, metadata):
    """Save checkpoint - includes all state fields."""
    # Checkpoint contains:
    # - messages (conversation history)
    # - current_input_tokens
    # - current_output_tokens
    # - total_cost
    # - files (virtual filesystem)
```

**Benefits:**
- ✅ Token counts survive session restarts
- ✅ Resume shows accurate usage
- ✅ Cost tracking across days
- ✅ Compression history preserved

---

### 7. Example: Real Session Flow

```
Session Start:
  context_window = 128000 (from config)
  current_input_tokens = None
  current_output_tokens = None
  
  Display: general:gpt-4
  ❯ _

User: "Analyze this file"
  → Agent invokes LLM with message history
  → LLM returns: usage_metadata = {input: 1234, output: 567}
  → State updated: current_input_tokens = 1234
  → State updated: current_output_tokens = 567
  → Total: 1234 + 567 = 1801
  → Percentage: (1801 / 128000) * 100 = 1.4%
  
  Display: general:gpt-4  [1.8K/128K tokens (1%)]
  ❯ _

... many exchanges later ...

User: "Continue the analysis"
  → Agent invokes LLM
  → LLM returns: usage_metadata = {input: 102400, output: 1200}
  → State updated: current_input_tokens = 102400
  → Total: 102400 + 1200 = 103600
  → Usage ratio: 103600 / 128000 = 0.81
  → 0.81 >= 0.8 threshold → AUTO-COMPRESS!
  
  Display: Context at 81%, auto-compressing...
  ⏱ Summarizing with haiku-4.5
  ⏱ Creating new thread: 7b3c9a2f
  
  Display: general:gpt-4  [5.2K/128K tokens (4%)]
  ❯ _
  
  Conversation continues with fresh context!
```

---

### 8. Key Utilities

#### **Token Formatting**

```python
# src/utils/cost.py
def format_tokens(tokens: int) -> str:
    """Format token count for display."""
    if tokens >= 1_000_000:
        return f"{tokens / 1_000_000:.1f}M"  # "1.2M"
    elif tokens >= 1_000:
        return f"{tokens / 1_000:.0f}K"       # "45K"
    else:
        return str(tokens)                     # "234"
```

#### **Percentage Calculation**

```python
# src/utils/cost.py
def calculate_context_percentage(current_tokens: int, context_window: int) -> float:
    """Calculate percentage of context window used."""
    if context_window <= 0:
        return 0.0
    return (current_tokens / context_window) * 100
```

---

### 9. Design Rationale

#### **Why Static Context Window?**
- ✅ **Simple**: No complex calculation needed
- ✅ **Accurate**: Based on official provider specs
- ✅ **Reliable**: Doesn't change during session
- ✅ **Provider-specific**: Different models have different limits

#### **Why LLM Metadata for Token Counting?**
- ✅ **Most accurate**: Provider's own tokenizer
- ✅ **Billing-aligned**: Matches what you're charged
- ✅ **Provider-specific**: Accounts for different tokenization
- ✅ **Includes special tokens**: System messages, formatting, etc.

#### **Why Multi-Level Fallback?**
- ✅ **Robustness**: Works even if LLM doesn't provide metadata
- ✅ **Tool outputs**: Can count tokens from tool responses
- ✅ **Compression**: Can estimate before compression
- ✅ **Graceful degradation**: Always returns a count

#### **Why Auto-Compression?**
- ✅ **Prevents errors**: Avoids context overflow
- ✅ **Seamless**: User doesn't need to manage context
- ✅ **Cost-effective**: Uses cheaper model for summarization
- ✅ **Preserves context**: Summary maintains key information

---

### Summary

| Aspect | Implementation | Source |
|--------|---------------|--------|
| **Context Window** | Static configuration | `config.llms.yml` |
| **Current Tokens** | Dynamic extraction | LLM `usage_metadata` |
| **Calculation** | Multi-level fallback | LLM → tiktoken → char/4 |
| **Display** | Real-time prompt | CLI placeholder |
| **Persistence** | SQLite storage | Checkpointer |
| **Monitoring** | Auto-compression | 80% threshold |
| **Accuracy** | Provider-specific | Each LLM's tokenizer |

**Philosophy**: Use the **most accurate source** (LLM's own count) with **graceful fallbacks** for robustness, enabling **smart auto-compression** before overflow occurs. 🎯

---

## State Management

### State Schema

**Base State** (`src/state/base.py`)

```python
class BaseState(AgentStatePydantic):
    # Inherited from AgentStatePydantic
    messages: list[AnyMessage]  # Conversation history
    
    # Custom fields
    todos: list[Todo] | None = None
    files: Annotated[dict[str, str], file_reducer] = Field(default_factory=dict)
    current_input_tokens: int | None = None
    current_output_tokens: int | None = None
    total_cost: float | None = None
```

**File Reducer** (merge virtual filesystem):
```python
def file_reducer(l, r):
    if l is None:
        return r
    elif r is None:
        return l
    else:
        return {**l, **r}  # Merge dicts
```

### State Flow

```
1. User Input
   ├─ HumanMessage added to state.messages
   └─ State passed to graph

2. Agent Node
   ├─ Reads state.messages
   ├─ Invokes LLM
   ├─ Returns AIMessage + usage metadata
   └─ State updated:
       ├─ messages.append(ai_message)
       ├─ current_input_tokens = X
       ├─ current_output_tokens = Y
       └─ total_cost += call_cost

3. Tool Node (if tool calls exist)
   ├─ Executes tools
   ├─ Returns ToolMessages
   └─ State updated:
       ├─ messages.append(tool_messages)
       └─ files = {**files, **new_files}  # If compressed

4. Checkpointer
   ├─ Saves state after each node
   └─ Keyed by (thread_id, checkpoint_id)

5. CLI Context Sync
   └─ Updates prompt display with latest tokens/cost
```

### Persistence

**SQLite Checkpointer** (`checkpointer/impl/sqlite.py`)

```python
class SQLiteSaver(BaseCheckpointSaver):
    """Persistent checkpoint storage."""
    
    def __init__(self, conn_string: str, max_connections: int = 10):
        self.conn_string = conn_string
        self.pool = create_pool(max_connections)
    
    async def aput(self, config, checkpoint, metadata):
        """Save checkpoint to SQLite."""
        # Insert into checkpoints table
        # Keyed by (thread_id, checkpoint_ns, checkpoint_id)
    
    async def aget_tuple(self, config):
        """Retrieve checkpoint from SQLite."""
        # Query latest checkpoint for thread
```

**Database Schema:**
```sql
CREATE TABLE checkpoints (
    thread_id TEXT,
    checkpoint_ns TEXT,
    checkpoint_id TEXT,
    parent_checkpoint_id TEXT,
    type TEXT,
    checkpoint BLOB,
    metadata BLOB,
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id)
);

CREATE TABLE writes (
    thread_id TEXT,
    checkpoint_ns TEXT,
    checkpoint_id TEXT,
    task_id TEXT,
    idx INTEGER,
    channel TEXT,
    type TEXT,
    value BLOB,
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id, task_id, idx)
);
```

**Thread Listing** (`checkpointer/utils.py`)

```python
async def list_threads(checkpointer, agent_name, limit=10):
    """Get recent threads for agent."""
    # Query checkpoints grouped by thread_id
    # Order by latest checkpoint timestamp
    # Return list of {thread_id, created_at, updated_at}
```

---

## Tool System

### Tool Types

**1. Implementation Tools** (`src/tools/impl/`)
- Built-in Python tools
- File operations, grep, terminal, web

**2. MCP Tools** (`src/mcp/`)
- External server tools
- Loaded via MCP protocol
- Examples: filesystem, github, postgres, etc.

**3. Internal Tools** (`src/tools/internal/`)
- System-level tools
- Memory operations
- Virtual filesystem access

**4. Subagent Tools** (`src/tools/subagents/`)
- Delegate tasks to specialized subagents
- Dynamic subagent invocation

### Tool Approval System

**Configuration** (`config.approval.json`)

```json
{
  "approval": {
    "always_approve": [
      "^read_file$",
      "^grep_search$",
      "^list_directory$"
    ],
    "always_deny": [
      "^execute_shell$.*rm -rf.*",
      "^write_file$.*\\.ssh.*"
    ]
  }
}
```

**Wrapper Implementation** (`tools/wrapper.py`)

```python
class ApprovalToolWrapper(BaseTool):
    """Wraps tool with approval logic."""
    
    async def _arun(self, **kwargs):
        tool_name = self.wrapped_tool.name
        
        # Check approval mode
        approval_mode = self.get_approval_mode()
        
        if approval_mode == ApprovalMode.AGGRESSIVE:
            # Bypass all rules
            return await self.wrapped_tool.ainvoke(kwargs)
        
        # Check always_deny patterns
        if self._matches_deny_rules(tool_name, kwargs):
            raise ToolException(f"Tool '{tool_name}' denied by approval rules")
        
        if approval_mode == ApprovalMode.ACTIVE:
            # Bypass always_approve rules
            return await self.wrapped_tool.ainvoke(kwargs)
        
        # SEMI_ACTIVE: Check always_approve patterns
        if self._matches_approve_rules(tool_name, kwargs):
            return await self.wrapped_tool.ainvoke(kwargs)
        
        # Request user approval
        approved = await self.prompt_for_approval(tool_name, kwargs)
        if approved:
            return await self.wrapped_tool.ainvoke(kwargs)
        else:
            raise ToolException(f"Tool '{tool_name}' rejected by user")
```

**Interrupt Handling** (`cli/interface/interrupts.py`)

```python
async def handle(self, interrupts: list[Interrupt]):
    """Handle tool approval interrupts."""
    for interrupt in interrupts:
        tool_call = interrupt.value
        
        # Display tool call info
        console.print(f"Tool: {tool_call['name']}")
        console.print(f"Args: {tool_call['args']}")
        
        # Prompt for approval
        choice = await self.prompt_for_choice(["Approve", "Reject"])
        
        if choice == "Approve":
            return tool_call  # Resume with approval
        else:
            return None  # Reject
```

### Tool Output Compression

**Problem**: Tool outputs can exceed context window

**Solution**: CompressingToolNode

```python
class CompressingToolNode(ToolNode):
    async def _arun_one(self, call, input_type, tool_runtime):
        # Execute tool
        result = await super()._arun_one(call, input_type, tool_runtime)
        
        # Check size
        max_tokens = tool_runtime.config.get("configurable", {}).get("tool_output_max_tokens")
        content = result.content
        token_count = calculate_message_tokens([HumanMessage(content=content)], self.model)
        
        if token_count > max_tokens:
            # Compress to virtual file
            file_id = f"tool_output_{result.tool_call_id}.txt"
            compressed_msg = ToolMessage(
                content=f"Output compressed to: {file_id}\nUse read_memory_file(file_id='{file_id}')",
                tool_call_id=result.tool_call_id,
                short_content="[Compressed]"
            )
            
            # Return Command with file storage
            return Command(
                update={
                    "messages": [compressed_msg],
                    "files": {file_id: content}
                }
            )
        
        return result
```

**Benefits:**
- Large outputs don't bloat context
- Agent can read via `read_memory_file()`
- Stored in state.files (persisted)

---

## Configuration System

### Configuration Files

**Location**: `.langrepl/` (in working directory)

```
.langrepl/
├── config.agents.yml       # Agent definitions
├── config.llms.yml         # LLM configurations
├── config.subagents.yml    # Subagent definitions
├── config.checkpointers.yml # Storage configurations
├── config.mcp.json         # MCP server configs
├── config.approval.json    # Tool approval rules
├── config.history          # Command history
├── langrepl.db             # SQLite checkpoints
└── prompts/
    └── user_memory.md      # User memory/instructions
```

### Agent Configuration

**`config.agents.yml`**

```yaml
agents:
  - name: general
    description: General-purpose agent
    llm: gpt-4                    # Default LLM
    checkpointer: sqlite
    recursion_limit: 30
    tool_output_max_tokens: 2000
    
    tools:                        # Tool patterns (glob)
      - impl:file_system:*
      - impl:grep_search:*
      - impl:web:*
      - mcp:*:*                   # All MCP tools
      - internal:memory:*
    
    subagents:                    # Subagent delegation
      - general-purpose
      - explorer
    
    compression:
      auto_compress_enabled: true
      auto_compress_threshold: 0.8
      compression_llm: haiku-4.5
    
    prompt: |
      You are a helpful AI assistant.
      Current date: {current_date_time_zoned}
      Working directory: {working_dir}
      {user_memory}
```

**Prompt Template Variables:**
- `{working_dir}`: Current working directory
- `{platform}`: OS platform (Windows, Linux, Darwin)
- `{os_version}`: OS version
- `{current_date_time_zoned}`: Current date/time with timezone
- `{user_memory}`: Contents of `prompts/user_memory.md`

### LLM Configuration

**`config.llms.yml`**

```yaml
llms:
  - alias: gpt-4
    provider: openai
    model: gpt-4
    max_tokens: 4096
    temperature: 0.7
    streaming: false
    context_window: 128000
    input_cost_per_mtok: 30.0
    output_cost_per_mtok: 60.0
    rate_config:
      requests_per_second: 5.0
      input_tokens_per_second: 50000.0
      output_tokens_per_second: 10000.0
      check_every_n_seconds: 1.0
      max_bucket_size: 10
  
  - alias: claude-sonnet
    provider: anthropic
    model: claude-3-5-sonnet-20241022
    max_tokens: 8192
    temperature: 1.0
    context_window: 200000
    input_cost_per_mtok: 3.0
    output_cost_per_mtok: 15.0
    extended_reasoning:      # Provider-agnostic thinking config
      type: extended_thinking
      budget_tokens: 10000
  
  - alias: o1-preview
    provider: openai
    model: o1-preview
    max_tokens: 32768
    temperature: 1.0
    context_window: 128000
    input_cost_per_mtok: 15.0
    output_cost_per_mtok: 60.0
    extended_reasoning:
      type: extended_thinking  # Maps to max_completion_tokens
```

### MCP Configuration

**`config.mcp.json`**

```json
{
  "mcpServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/path/to/allowed/dir"]
    },
    "github": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-github"],
      "env": {
        "GITHUB_PERSONAL_ACCESS_TOKEN": "your_token"
      }
    },
    "postgres": {
      "command": "uvx",
      "args": ["mcp-server-postgres", "postgresql://user:pass@localhost/db"]
    }
  }
}
```

### Approval Rules

**`config.approval.json`**

```json
{
  "approval": {
    "always_approve": [
      "^read_file$",
      "^grep_search$",
      "^list_directory$",
      "^web_search$"
    ],
    "always_deny": [
      "^execute_shell$.*rm -rf.*",
      "^write_file$.*\\.ssh.*",
      "^execute_shell$.*sudo.*"
    ]
  }
}
```

**Pattern Matching:**
- Regex patterns applied to `tool_name` and stringified `kwargs`
- `always_deny` checked first (highest priority)
- `always_approve` checked next
- Unmatched tools prompt for approval (unless approval mode overrides)

---

## Key Design Patterns

### 1. Factory Pattern

**Used extensively for component creation:**

```python
# GraphFactory
graph = await graph_factory.create(agent_config, schema, mcp_config)

# AgentFactory
agent = agent_factory.create(tools, llm, prompt, subagents)

# LLMFactory
llm = llm_factory.create(llm_config)

# ToolFactory
tools = tool_factory.get_impl_tools()

# MCPFactory
mcp_client = await mcp_factory.create(mcp_config)

# CheckpointerFactory
checkpointer = checkpointer_factory.create(checkpointer_config, db_path)
```

**Benefits:**
- Centralized creation logic
- Easy to extend with new providers
- Configuration-driven instantiation

### 2. Context Manager Pattern

**Used for resource lifecycle:**

```python
# Graph context
async with initializer.get_graph(agent, model, working_dir) as graph:
    # Use graph
    pass
# Graph and dependencies automatically cleaned up

# Checkpointer context
async with checkpointer_factory.create(config, db_path) as checkpointer:
    # Use checkpointer
    pass
# Connections closed
```

### 3. Streaming Pattern

**Async generators for incremental results:**

```python
async for chunk in graph.astream(input, config, stream_mode="updates"):
    # Process chunk immediately
    await handle_chunk(chunk)
# No buffering, real-time display
```

### 4. Command Pattern

**LangGraph Command for state updates:**

```python
# Return Command instead of dict to update state
return Command(
    update={"messages": [msg], "files": {file_id: content}},
    resume=resume_value  # For interrupt resumption
)
```

### 5. Strategy Pattern

**Different agent strategies:**

```python
if subagents:
    return create_deep_agent(...)  # Deep agent strategy
else:
    return create_react_agent(...)  # ReAct agent strategy
```

### 6. Decorator Pattern

**Tool wrapping with approval:**

```python
# Original tool
original_tool = ReadFileTool()

# Wrapped with approval logic
wrapped_tool = ApprovalToolWrapper(
    wrapped_tool=original_tool,
    approval_config=approval_config
)
```

### 7. Observer Pattern

**Context synchronization:**

```python
# Agent updates state
state.current_input_tokens = 1000

# CLI observes state changes
session.update_context(current_input_tokens=1000)

# Prompt reflects changes
prompt.refresh_style()  # Display updated tokens
```

### 8. Template Method Pattern

**Message rendering:**

```python
def render_message(message):
    # Template method
    if isinstance(message, HumanMessage):
        render_user_message(message)  # Specific implementation
    elif isinstance(message, AIMessage):
        render_assistant_message(message)  # Specific implementation
    # ...
```

### 9. Singleton Pattern

**Global settings:**

```python
# src/core/settings.py
settings = Settings()  # Single instance

# Usage everywhere
from src.core.settings import settings
log_level = settings.log_level
```

### 10. Builder Pattern

**Graph construction:**

```python
workflow = StateGraph(state_schema)
workflow.add_node("agent", agent_func)
workflow.add_node("tools", tool_func)
workflow.add_conditional_edges("agent", should_continue, ["tools", END])
workflow.set_entry_point("agent")
return workflow.compile(checkpointer=checkpointer)
```

---

## Summary

### Observability Features

✅ **Logging**: Structured logging with configurable levels  
✅ **Tracing**: LangSmith integration (passive, env-based)  
✅ **Token Tracking**: Real-time usage extraction from LLM responses  
✅ **Cost Calculation**: Per-call and cumulative cost tracking  
✅ **Performance Timing**: Startup phase timing with `-t` flag  
✅ **Context Monitoring**: Auto-compression on threshold  
✅ **Error Handling**: Multi-layer error capture and display  

### Key Strengths

1. **Clean Separation**: Input (prompt_toolkit) / Execution (LangGraph) / Output (Rich)
2. **Streaming Architecture**: Real-time response rendering
3. **Persistent State**: SQLite checkpointing for conversation history
4. **Extensible Tools**: Easy to add impl/MCP/internal tools
5. **Multi-Provider LLM**: Unified interface for 8+ providers
6. **Human-in-the-Loop**: Flexible approval system
7. **Cost Awareness**: Live token/cost tracking
8. **Auto-Compression**: Smart context window management
9. **Subagent Delegation**: Task routing to specialized agents
10. **Configuration-Driven**: YAML/JSON configs, no code changes needed

### Technology Stack

| Layer | Technology |
|-------|-----------|
| **CLI Input** | prompt_toolkit, argparse |
| **CLI Output** | Rich, Pygments |
| **Agent Execution** | LangGraph, LangChain |
| **LLM Integration** | langchain-* (8+ providers) |
| **State Management** | Pydantic, SQLite |
| **Tool Protocol** | MCP (Model Context Protocol) |
| **Observability** | LangSmith, Python logging |
| **Configuration** | Pydantic Settings, YAML, JSON |
| **Async Runtime** | asyncio, aiofiles, aiosqlite |

---

**This documentation provides a comprehensive understanding of the Langrepl architecture, from high-level design to implementation details. Use it as a reference for extending, debugging, or contributing to the system.**
